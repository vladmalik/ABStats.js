<!DOCTYPE html>
<html>
	<head>
		<meta name="description" content="">
		<meta charset="utf-8">
		<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
		<script src="../abstats.js"></script>
		<link rel="stylesheet" href="../style.css">		
	</head>
	<body class="report">
		<section class="recommendation layout-contrast">
			<h1>Aim for <output class='sample-total-plan'></output> total visitors</h1>
			<p class="text-short">That's <em><output class='sample-variation-plan'></output> per variant</em> in an A/B comparison. Expect <em class="weeks-plan"><output class='output-weeks'></output> weeks</em> duration. You can adjust this duration below and see the impact on test sensitivity.</p>
			<div class="chart-type-bar">
				<div class="chart">
					<div class="row" style="display: none">
						<span class="label-y"><small>Your plan</small></span>
						<span class="chart-bar-container chart-bar-plan">
							<span class="label-x">
								<span class="likelihood"><em></em></span>
							</span>
							<span style="width: 100%" class="chart-bar"></span>
						</span>
					</div>
					<div class="row">
						<span class="label-y"><small>Suggested</small></span>
						<span class="chart-bar-container chart-bar-recommended">
							<span class="label-x">
								<span class="likelihood"><em></em></span>
							</span>
							<span style="width: 100%" class="chart-bar selected"></span>
						</span>
					</div>
					<div class="row">
						<span class="label-y"><small>Maximum</small></span>
						<span class="chart-bar-container chart-bar-ideal">
							<span class="label-x">
								<span class="likelihood"><em></em></span>
							</span>
							<span style="width: 100%;" class="chart-bar"></span>
						</span>
					</div>
				</div>
				<p class="chart-assumptions">
					<span><em>Assuming site holds at </em></span>
					<span class="rate"><em><output class="output-rate">5</output>%</em> conversion rate</span>
					<span class="traffic"><em><output class="output-traffic">5,000</output></em> people/week</span>
				</p>				
			</div>
		</section>
		<section class="report-sensitivity state-sensitivity">
			<h2>What if there is a true effect</h2>			
			<form class="group-inputs">				
				<p>Here are effect sizes detectible within your <em>time constraint</em>. Each row makes a <em>separate assumption</em> about the true size of the effect, assuming one exists. The rows altogether represent uncertainty about expected effect. Rows below your target effect show how much sensitivity is reduced if you overestimate the true effect.</p>
				<table class="form-duration">
					<thead>
						<tr>
							<th><label>Weeks duration</label></th>
							<th><label>Participants <em>per variant</em></label></th>
							<th><label>Chart Shows</label></th>
						</tr>
					</thead>
					<tbody>
						<tr>
							<td><input type="text" maxlength="4" value="2" class="input-weeks context-sensitivity"></td>
							<td><output class="output-sample"></output></td>
							<td class="chart-sensitivity-state"><a class="selected change-state-sensitivity">Sensitivity</a> or <a class="change-state-error">Margin of Error</a></td>
						</tr>							
					</tbody>
				</table>
			</form>
			<div class="chart-type-bar">
				<p class='label-chart'><span class="chart-state chart-state-sensitivity">Chance of statistically significant outcome if effect exists:</span><span class="chart-state chart-state-error">Actual result to expect IF the true effect is...</span></p>
				<div class="chart"></div>
				<div class="chart-warnings">
					<p class='warning warning-lowpower' style='display: none'><span class="symbol-warning">&#9888;</span> The test doesn't have enough sensitivity (power) to detect reasonably smaller effects. To detect a smaller effect, (a) increase test duration, (b) test something else, (c) change your metric, or (d) implement change, wait, and compare to past metrics.</p>
					<p class='warning warning-toolong' style='display: none'><span class="symbol-warning">&#9888;</span> Your test may be too long. Consider the opportunity cost and risks of running multi-month tests. Try choosing a primary metric with a higher rate in order to reduce duration.</p>
				</div>	
				<p class="chart-assumptions">				
					<span><em>Assumptions:</em> True effect exists</span>
					<span class="rate"><em><output class="output-rate"></output>%</em> conversion rate</span>
					<span class="traffic"><em><output class="output-traffic"></output></em> people/week</span>
				</p>			
			</div>
			<p class="p-details  chart-state chart-state-sensitivity"><em>About Sensitivity:</em> The sensitivity chart shows you the probablity of a statistically significant outcome (power) for each possible true effect size. This is "power" or 1-beta in typical sample size calculators. The optimal power recommended by statisticians is 80% (16 in 20 chance). This means if you ran this experiment over and over, and the true effect is exactly what you expected, you will fail to detect a statistically significant result about 20% of the time.</p>
			<p class="p-details chart-state-error chart-state"><em>About Margin of Error:</em> Even if you're spot on about the true effect size, the actual outcome will vary by chance. This view gives you the 80% Confidence Interval for expected actual outcome. The mid-point is most likely, but anything in the range is consistent with a true effect of selected size. An actual result would fall outside this range about 20% of the time. <em>To narrow the margin of error</em>, increase experiment duration. If you detect a statistically strong effect smaller than expected or observe an effect far sooner than expected, you should be skeptical.</p>
		</section>
		<section class="report-falsepositive">
			<h2>What if there is no true effect</h2>
			<p>If your Variant makes no difference, here is a range of effects you would expect to see <em>just by chance</em>:</p>
			<form class="group-inputs">			
				<table class="form-duration">
					<thead>
						<tr>
							<th><label>Weeks duration</label></th>
							<th><label>Participants <em>per variant</em></label></th>
						</tr>
					</thead>
					<tbody>
						<tr>
							<td><input type="text" maxlength="4" value="2" class="input-weeks context-falsepositive"></td>
							<td><output class="output-sample"></output></td>
						</tr>							
					</tbody>
				</table>
			</form>	
			<div class="chart chart-type-bar">
				<p class='label-chart'>Chance of seeing a false positive of <span style='text-decoration: underline'>at least</span>...</p>
				<div class="chart"></div>
				<div class="chart-warnings">
					<p class='warning warning-falsepositive' style='display: none'><span class="symbol-warning">&#9888;</span> Your risk of seeing large effects just by chance is a tad high. If you are doing exploratory testing, increase duration to reduce the risk of getting large false positives.</p>
				</div>
				<p class="chart-assumptions">				
					<span><em>Assumptions:</em> No true difference</span>
					<span class="rate"><em><output class="output-rate"></output>%</em> conversion rate</span>
					<span class="traffic"><em><output class="output-traffic"></output></em> people/week</span>
				</p>					
			</div>
			<p class="p-details"><em>About False Positives:</em> Since you don't know if there is an effect or not, make sure you have high sensitivity to detect a true effect if it's there but a low risk of detecting a similar effect just by chance. Expect a positive on <em>1 of every 20 comparisons </em> (5%). For example, if you run a test with A + 4 variations, your risk of seeing a false positive in that test is 5%x4 = 20%. This report assumes you will call "statistically significant" any result with a p-value of 0.05 or lower. This fixes your false positive risk at 1 in 20 <em>regardless of how long you run your test</em>. However, the effect size of a false positives decreases with greater sample size. <em>If you peek</em> as your tests run AND decide to keep going based on interim performance, then your false positive risk will increase the more you do this.</p>
		</section>
		<section>
			<center>
				You can use <em>UP/DOWN KEYS</em> (or +/-) to change weeks.
			</center>	
		</section>
		<section class="layout-contrast">
			<div><h3>Share this report: </h3> <output id="sharelink"></output></div>
		</section>
		<footer>
			<p>All calculations done with the easy-to-use <a href="https://github.com/vladmalik/ABStats.js">ABStats.js</a>.</p>
			<p>Project by <a href="http://twitter.com/vladmalik">Vlad Malik</a></p>
		</footer>
		<script type="text/javascript">
		  WebFontConfig = {
			google: { families: [ 'Open+Sans:400,300,700:latin', 'Roboto:400,300,700:latin' ] }
		  };
		  (function() {
			var wf = document.createElement('script');
			wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
			  '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
			wf.type = 'text/javascript';
			wf.async = 'true';
			var s = document.getElementsByTagName('script')[0];
			s.parentNode.insertBefore(wf, s);
		  })(); 
		</script>
		<script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-54458193-2', 'auto');
		  ga('send', 'pageview');

		</script>		  
		  
		<script src="reports.js"></script>
	</body>

</html>
